{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Airline Reviews: Topic & Sentiment Analysis**\n",
        "This notebook performs topic modelling and sentiment classification on airline customer reviews."
      ],
      "metadata": {
        "id": "nXeKPGQ89AUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoA41CDLVTO5"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q bertopic[visualization] sentence-transformers umap-learn hdbscan transformers torch datasets nltk matplotlib wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry pandas"
      ],
      "metadata": {
        "id": "tlc4Jm0t2Qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "hi_7twTRXFYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords if not already installed\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "JAcLCalkD-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b20X-xhnY23j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned airline reviews dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/airline_reviews_cleaned_dataset.csv')\n",
        "# Keep only the 'customer_review' column, drop missing values, and truncate long reviews\n",
        "texts = df['customer_review'].dropna().astype(str).str[:800].tolist()"
      ],
      "metadata": {
        "id": "T4NRcTaiXGm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pycountry\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Download NLTK stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load city names from GitHub dataset\n",
        "url = \"https://raw.githubusercontent.com/datasets/world-cities/master/data/world-cities.csv\"\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    data = StringIO(response.text)\n",
        "    df_cities = pd.read_csv(data)\n",
        "    city_names = df_cities['name'].str.strip().str.lower().unique().tolist()\n",
        "except Exception as e:\n",
        "    print(\"Error loading city data:\", e)\n",
        "    city_names = []\n",
        "\n",
        "# Get all country names\n",
        "country_names = [country.name for country in pycountry.countries]\n",
        "\n",
        "# Define custom stopwords\n",
        "custom_stopwords = ['Business Class','Economy Class','First Class','Premium Economy', 'flight', 'flights', 'airline', 'airlines', 'bag', 'boarding', 'check','carry', 'staff', 'united', 'american', 'delta', 'wizz', 'easyjet','ryanair', 'airport', 'passenger', 'travel', 'ticket', 'seat', 'gate','name', 'trip', 'verified','adria', 'aegean', 'aer', 'lingus', 'aeroflot', 'russian', 'aeromexico','air', 'arabia', 'canada', 'france', 'new', 'zealand', 'airasia', 'alaska','alitalia', 'ana', 'all', 'nippon', 'asiana', 'austrian', 'avianca','bangkok', 'british', 'brussels', 'china', 'eastern', 'southern', 'copa','egyptair', 'emirates', 'ethiopian', 'etihad', 'eurowings', 'eva','finnair', 'flydubai', 'frontier', 'garuda', 'indonesia', 'germanwings','gulf', 'iberia', 'icelandair', 'indigo', 'jetblue', 'klm', 'royal','dutch', 'korean', 'kuwait', 'latam', 'lot', 'polish', 'lufthansa','norwegian', 'pegasus', 'qantas', 'qantaslink', 'qatar', 'maroc','jordanian', 'sas', 'scandinavian', 'saudi', 'arabian', 'singapore','south', 'african', 'southwest', 'spirit', 'sunwing', 'swiss', 'intl','tap', 'portugal', 'tarom', 'romanian', 'thai', 'smile', 'tunisair','turkish', 'ukraine', 'international', 'virgin', 'america', 'vueling','wow', 'lines', 'airways']\n",
        "\n",
        "# Add NLTK English stopwords\n",
        "custom_stopwords += list(stopwords.words('english'))\n",
        "\n",
        "# Add country and city names\n",
        "custom_stopwords += country_names\n",
        "custom_stopwords += city_names\n",
        "\n",
        "# Remove duplicates\n",
        "custom_stopwords = list(set(custom_stopwords))  # Ensures uniqueness\n",
        "\n",
        "# Load embedding model (moved here for better flow)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "print(f\"Total stopwords loaded: {len(custom_stopwords)}\")"
      ],
      "metadata": {
        "id": "PUUeNHFv4nRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure CountVectorizer to remove stopwords and create n-grams (1-2 word phrases)\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=custom_stopwords,\n",
        "    min_df=5 # Ignore words that appear in fewer than 5 reviews\n",
        ")\n",
        "\n",
        "# Configure UMAP for dimensionality reduction before clustering\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=15,     # Number of neighbors considered during embedding\n",
        "    n_components=5,     # Reduce to 5 dimensions\n",
        "    min_dist=0.0,       # Minimum distance between points\n",
        "    metric='cosine',    # Similarity measure\n",
        "    random_state=42     # Reproducibility\n",
        ")\n",
        "\n",
        "# Configure HDBSCAN for hierarchical clustering of similar reviews\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=30,              # Minimum size of a cluster\n",
        "    metric='euclidean',               # Distance metric\n",
        "    cluster_selection_method='eom',   # Method for selecting clusters\n",
        "    prediction_data=True              # Enables representative docs\n",
        ")"
      ],
      "metadata": {
        "id": "ZjjcXWA7XJZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BERTopic with all custom models and settings\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,  # Model used for embeddings\n",
        "    vectorizer_model=vectorizer_model,  # Custom stopwords and n-grams\n",
        "    umap_model=umap_model,            # Dimensionality reduction\n",
        "    hdbscan_model=hdbscan_model,      # Clustering algorithm\n",
        "    language=\"english\",               # Language of the text\n",
        "    calculate_probabilities=True,     # Get topic probabilities per document\n",
        "    verbose=True                      # Print progress\n",
        ")"
      ],
      "metadata": {
        "id": "8jgYeLsTXR6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = topic_model.fit_transform(texts)\n",
        "df['topic'] = topics"
      ],
      "metadata": {
        "id": "r_UOOlk6ABXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info[['Topic', 'Name', 'Count']])"
      ],
      "metadata": {
        "id": "d2O_QbpD-H6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out outlier reviews (topic = -1)\n",
        "df_filtered = df[df['topic'] != -1]\n",
        "texts_filtered = df_filtered['customer_review'].tolist()\n",
        "topics_filtered = df_filtered['topic'].tolist()"
      ],
      "metadata": {
        "id": "j8M9eYH267dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a lightweight RoBERTa-based sentiment classifier from Hugging Face\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a Hugging Face pipeline for sentiment analysis\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
        ")\n",
        "\n",
        "# Map output labels to readable sentiment names\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"negative\",\n",
        "    \"LABEL_1\": \"neutral\",\n",
        "    \"LABEL_2\": \"positive\"\n",
        "}\n",
        "\n",
        "# Classify each review in batches to avoid memory overload\n",
        "batch_size = 32\n",
        "sentiments = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch = texts[i:i+batch_size]\n",
        "    results = sentiment_pipeline(batch)\n",
        "    sentiments.extend([label_map[res['label']] for res in results])\n",
        "\n",
        "# Add sentiment labels to the DataFrame\n",
        "df['sentiment'] = sentiments"
      ],
      "metadata": {
        "id": "9xA7iw6rXgE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group reviews by topic and compute sentiment distribution\n",
        "topic_summary = df.groupby('topic').agg(\n",
        "    review_count=('customer_review', 'count'),\n",
        "    positive=('sentiment', lambda x: (x == 'positive').mean()),\n",
        "    neutral=('sentiment', lambda x: (x == 'neutral').mean()),\n",
        "    negative=('sentiment', lambda x: (x == 'negative').mean()),\n",
        "    top_sentiment=('sentiment', lambda x: x.value_counts().idxmax())\n",
        ").reset_index()\n",
        "\n",
        "# Get automatically generated topic names from BERTopic\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Merge topic names into summary table\n",
        "topic_summary = topic_summary.merge(topic_info[['Topic', 'Name']], left_on='topic', right_on='Topic')\n",
        "topic_summary = topic_summary.drop(columns=['Topic'])\n",
        "\n",
        "# Sort by most negative sentiment for pain point identification\n",
        "topic_summary.sort_values('negative', ascending=False, inplace=True)\n",
        "\n",
        "# Show top 10 most negative topics\n",
        "print(\"Top 10 Topics by Negative Sentiment\")\n",
        "topic_summary.head(10)"
      ],
      "metadata": {
        "id": "PNDChFw6XheQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top words per topic\n",
        "print(topic_info[['Topic', 'Name', 'Count']])"
      ],
      "metadata": {
        "id": "OAxFoyeh2otM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the most negative topics using a bar chart\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=topic_summary.head(10), x='Name', y='negative', palette='Reds_r')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Top 10 Topics by % Negative Sentiment')\n",
        "plt.ylabel('% Negative Reviews')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mecNWPSsXl8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve example reviews by topic and sentiment\n",
        "def get_example_reviews(df, topic_id, sentiment, n=3):\n",
        "    return df[(df['topic'] == topic_id) & (df['sentiment'] == sentiment)]['customer_review'].sample(n, replace=True).tolist()\n",
        "\n",
        "# Example: Show 3 negative reviews from the most negative topic\n",
        "most_negative_topic = topic_summary.iloc[0]['topic']\n",
        "print(\"Negative Reviews for Topic\", most_negative_topic)\n",
        "print(get_example_reviews(df, most_negative_topic, 'negative'))"
      ],
      "metadata": {
        "id": "nuRoKRP8XoYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_review(review_text, topic_model, sentiment_pipeline):\n",
        "    \"\"\"Classify a given review into topic and sentiment\"\"\"\n",
        "    # Get topic\n",
        "    topic_id, probs = topic_model.transform([review_text])\n",
        "    topic_words = topic_model.get_topic(topic_id[0])\n",
        "    topic_name = \", \".join([word for word, _ in topic_words]) if topic_words else \"Unknown/Outlier\"\n",
        "\n",
        "    # Get sentiment\n",
        "    sentiment_result = sentiment_pipeline([review_text])[0]\n",
        "    sentiment_label = sentiment_result['label']\n",
        "    sentiment_score = sentiment_result['score']\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nüìù Review:\")\n",
        "    print(review_text)\n",
        "    print(\"\\nüè∑Ô∏è Predicted Topic ID:\", topic_id[0])\n",
        "    print(\"üî§ Topic Name:\", topic_name)\n",
        "    print(\"\\nüòÉ Sentiment:\", label_map[sentiment_label])\n",
        "    print(\"üìä Confidence Score:\", round(sentiment_score, 4))"
      ],
      "metadata": {
        "id": "51FIzTQjwCCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask user if they want to enter their own review or use a random one\n",
        "choice = input(\"Do you want to enter your own review? (y/n): \").strip().lower()\n",
        "\n",
        "if choice == 'y':\n",
        "    user_input = input(\"Enter your review: \")\n",
        "    test_review(user_input, topic_model, sentiment_pipeline)\n",
        "else:\n",
        "    # Pick a random review from the DataFrame\n",
        "    random_index = random.randint(0, len(df) - 1)\n",
        "    random_review = df.iloc[random_index]['customer_review']\n",
        "    print(f\"\\nüî¢ Randomly selected index: {random_index}\")\n",
        "    test_review(random_review, topic_model, sentiment_pipeline)"
      ],
      "metadata": {
        "id": "X8GCfWmqC9uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_sentiment(df, topic_id, sentiment='negative', n=3):\n",
        "    examples = df[(df['topic'] == topic_id) & (df['sentiment'] == sentiment)]['customer_review'].sample(n, replace=True)\n",
        "    print(f\"\\nüìò Reviews where customers felt '{sentiment}' about topic {topic_id}:\")\n",
        "    for i, rev in enumerate(examples):\n",
        "        print(f\"{i+1}. {rev[:200]}...\")  # First 200 chars only"
      ],
      "metadata": {
        "id": "hmED38gjUbg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_negative_topic = topic_summary.iloc[0]['topic']\n",
        "explain_sentiment(df, most_negative_topic, 'negative')"
      ],
      "metadata": {
        "id": "jNyvb9SxUdwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insight_rows = []\n",
        "\n",
        "for _, row in topic_summary.iterrows():\n",
        "    topic_id = row['topic']\n",
        "    name = row['Name']\n",
        "    sentiment = row['top_sentiment']\n",
        "    count = row['review_count']\n",
        "\n",
        "    try:\n",
        "        example_reviews = df[(df['topic'] == topic_id) & (df['sentiment'] == sentiment)]['customer_review'].sample(min(2, count)).tolist()\n",
        "    except:\n",
        "        example_reviews = [\"No sample available\"]\n",
        "\n",
        "    insight_rows.append({\n",
        "        \"Topic ID\": topic_id,\n",
        "        \"Topic Name\": name,\n",
        "        \"Most Common Sentiment\": sentiment,\n",
        "        \"Review Count\": count,\n",
        "        \"Examples\": example_reviews\n",
        "    })\n",
        "\n",
        "insight_df = pd.DataFrame(insight_rows)\n",
        "insight_df.sort_values('Review Count', ascending=False, inplace=True)\n",
        "\n",
        "# Export to CSV for reporting\n",
        "insight_df.to_csv(\"topic_insights_with_reasons.csv\", index=False)\n",
        "\n",
        "# Display top 10 insights\n",
        "insight_df.head(10)"
      ],
      "metadata": {
        "id": "5chBdHA9UhhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_wordcloud_for_topic(df, topic_number):\n",
        "    reviews = df[df['topic'] == topic_number]['customer_review'].dropna()\n",
        "    text = \" \".join(reviews.astype(str).str.lower())\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=50).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Common Words in Topic {topic_number}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U0vOrQ6xUj-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud_for_topic(df, most_negative_topic)"
      ],
      "metadata": {
        "id": "PNbXr3bJUmGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Overview of Methodology\n",
        "\n",
        "This study employed a hybrid natural language processing approach combining topic modeling using BERTopic and sentiment classification using RoBERTa to analyze over 50,000 airline customer reviews. The primary objective was to uncover recurring themes within the reviews and assess the overall sentiment associated with each theme. This dual-layered analysis enabled the identification of key pain points experienced by customers, offering actionable insights for service improvement.\n",
        "\n",
        "The methodology involved several stages: data preprocessing, embedding generation using sentence transformers, topic modeling, sentiment classification, and result aggregation. Custom stopwords were used to refine the topic extraction process, ensuring domain-specific noise such as \"flight\" and \"luggage\" did not dominate the results. Sentiment classification was performed using a lightweight version of the RoBERTa model fine-tuned on Twitter sentiment data.\n",
        "\n",
        "### 2. Topic Modeling Output\n",
        "\n",
        "Through the use of BERTopic, more than 100 topics were extracted from the dataset. Each topic was represented by a set of keywords derived using c-TF-IDF weighting, allowing for meaningful interpretation of the underlying themes.\n",
        "\n",
        "Some of the most prominent topics included:\n",
        "- Norwegian flight cancellations and refund issues\n",
        "- Lost luggage and baggage claim problems\n",
        "- Delays and rebooking frustrations\n",
        "- Complaints about Spirit Airlines‚Äô service\n",
        "- Issues with boarding passes and staff assistance\n",
        "\n",
        "These topics provided an overview of the most commonly discussed aspects of the customer experience across various airlines.\n",
        "\n",
        "### 3. Sentiment Analysis per Topic\n",
        "\n",
        "Each review was classified into one of three sentiment categories: positive, neutral, or negative. Aggregating this data at the topic level revealed which themes were associated with the strongest negative emotions.\n",
        "\n",
        "One of the most notable findings was that the topic related to Norwegian Airline‚Äôs Gatwick refund issues had a **100% negative sentiment**. Other highly negative topics included complaints about lost luggage, delays, poor communication, and inadequate customer support.\n",
        "\n",
        "Across the top ten most negative topics, the percentage of negative sentiment ranged from **87.6% to 100%**, indicating widespread dissatisfaction among passengers regarding these specific areas of service.\n",
        "\n",
        "### 4. Visualization: Bar Chart of Top Negative Topics\n",
        "\n",
        "A bar chart was generated to visually represent the percentage of negative sentiment across the top ten topics. This visualization highlighted the severity of customer dissatisfaction linked to:\n",
        "- Flight cancellations and rebooking issues\n",
        "- Baggage mishandling\n",
        "- Poor customer service\n",
        "- Unexpected fees and lack of transparency\n",
        "- Long wait times and language barriers during support calls\n",
        "\n",
        "The chart served as a powerful tool for identifying priority areas that require immediate attention from airline management.\n",
        "\n",
        "### 5. Qualitative Insights from Reviews\n",
        "\n",
        "Three example reviews were selected from the most negatively rated topic, which revolved around Norwegian Airlines' cancellation policies and customer service failures.\n",
        "\n",
        "The first review detailed a situation where a flight was canceled at the last minute, resulting in a four-hour wait to reach a representative. Passengers were eventually rebooked but faced further complications including overbooking and delayed baggage.\n",
        "\n",
        "Another review described being left without clear information after a flight cancellation, leading to costly alternative arrangements and a complete loss of trust in the airline.\n",
        "\n",
        "The third review criticized the frequent delays and lack of proactive communication, suggesting that the airline's operational inefficiencies have long-term implications for brand loyalty.\n",
        "\n",
        "These qualitative examples reinforced the quantitative findings, showing that customer dissatisfaction often stems from a combination of poor communication, lack of empathy, and systemic operational issues.\n",
        "\n",
        "### 6. Discussion\n",
        "\n",
        "The combined results of topic modeling and sentiment classification offer deep insights into the customer experience within the airline industry.\n",
        "\n",
        "Key findings include:\n",
        "- Norwegian and Spirit Airlines received the highest number of negative reviews, particularly concerning cancellations, refunds, and customer service.\n",
        "- Lost luggage and baggage handling consistently appeared as major concerns across multiple topics.\n",
        "- Communication breakdowns during disruptions led to increased frustration, even when alternative flights were offered.\n",
        "- Overbooking practices and unclear refund policies contributed significantly to customer distrust.\n",
        "\n",
        "To improve passenger satisfaction, airlines should focus on enhancing transparency during disruptions, improving multilingual support systems, refining overbooking strategies, and investing in better baggage tracking technologies.\n",
        "\n",
        "### 7. Conclusion\n",
        "\n",
        "This research successfully applied advanced NLP techniques to extract valuable insights from unstructured customer feedback. By integrating topic modeling and sentiment classification, it identified critical areas where airlines can take corrective actions to enhance customer experience.\n",
        "\n",
        "Future research could expand this framework to enable real-time sentiment monitoring, integrate feedback analysis with CRM systems, or develop predictive models to anticipate customer churn based on sentiment trends."
      ],
      "metadata": {
        "id": "EmymgK9J_s2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aspects = [\n",
        "    # Existing ones\n",
        "    \"service\", \"staff\", \"crew\", \"attitude\",\n",
        "    \"luggage\", \"baggage\", \"lost\", \"claim\",\n",
        "    \"flight\", \"delay\", \"cancellation\", \"late\",\n",
        "    \"boarding\", \"process\", \"gate\", \"check-in\",\n",
        "    \"seat\", \"comfort\", \"legroom\", \"space\",\n",
        "    \"food\", \"meal\", \"snack\", \"drink\",\n",
        "    \"price\", \"cost\", \"value\", \"money\",\n",
        "    \"cleanliness\", \"dirty\", \"hygiene\",\n",
        "    \"entertainment\", \"wifi\", \"tv\", \"screen\",\n",
        "    \"communication\", \"update\", \"notice\",\n",
        "    \"ground service\", \"check in\",\"in-flight entertainment\", \"cabin crew\", \"flight path\", \"flight delay\",\n",
        "    \"refund policy\", \"rebooking\", \"online check-in\"]"
      ],
      "metadata": {
        "id": "M57kROH3Vvsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_aspects(review_text, aspect_list):\n",
        "    \"\"\"Detect which aspects are mentioned in a review\"\"\"\n",
        "    review_text = review_text.lower()\n",
        "    matched = []\n",
        "    for aspect in aspect_list:\n",
        "        if re.search(r'\\b' + re.escape(aspect) + r'\\b', review_text):\n",
        "            matched.append(aspect)\n",
        "    return list(set(matched))"
      ],
      "metadata": {
        "id": "5zjA8sc1Wnv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_review_with_aspects(review_text, topic_model, sentiment_pipeline, aspects):\n",
        "    # Truncate review to 510 tokens max (leave room for [CLS] and [SEP])\n",
        "    truncated_review = review_text[:510]\n",
        "\n",
        "    # Get topic\n",
        "    topic_id, probs = topic_model.transform([truncated_review])\n",
        "    topic_name = topic_model.get_topic_info().iloc[topic_id[0]]['Name']\n",
        "\n",
        "    # Get sentiment\n",
        "    sentiment_result = sentiment_pipeline(truncated_review)[0]\n",
        "    sentiment_label = label_map[sentiment_result['label']]\n",
        "    sentiment_score = sentiment_result['score']\n",
        "\n",
        "    # Extract aspects from review\n",
        "    detected_aspects = extract_aspects(review_text, aspects)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nüìù Review:\")\n",
        "    print(review_text)\n",
        "\n",
        "    print(\"üî§ Topic Name:\", topic_name)\n",
        "\n",
        "    print(\"\\nüòÉ Sentiment:\", sentiment_label)\n",
        "    print(\"üìä Confidence Score:\", round(sentiment_score, 4))\n",
        "\n",
        "    print(\"\\nüîç Detected Aspects Mentioned:\")\n",
        "    if detected_aspects:\n",
        "        print(\"- \" + \"\\n- \".join(detected_aspects))\n",
        "    else:\n",
        "        print(\"No clear aspects detected.\")\n",
        "\n",
        "    print(\"\\nüìå Summary Explanation:\")\n",
        "    if detected_aspects:\n",
        "        print(f\"This review is {sentiment_label} due to issues with: {', '.join(detected_aspects)}\")\n",
        "    else:\n",
        "        print(f\"This review is {sentiment_label}, but no specific aspect could be identified.\")"
      ],
      "metadata": {
        "id": "Ewr-9H3oWpmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = random.randint(0, len(df) - 1)\n",
        "random_review = df.iloc[random_index]['customer_review']\n",
        "analyze_review_with_aspects(random_review, topic_model, sentiment_pipeline, aspects)"
      ],
      "metadata": {
        "id": "q-Pu-lXiWrbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['detected_aspects'] = df['customer_review'].apply(lambda x: extract_aspects(x, aspects))"
      ],
      "metadata": {
        "id": "NbqxByAUWtZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_sentiment = df.explode('detected_aspects').dropna(subset=['detected_aspects'])\n",
        "aspect_sentiment.groupby(['detected_aspects', 'sentiment']).size().unstack(fill_value=0)"
      ],
      "metadata": {
        "id": "j5uUMq5iZfgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_counts = df.explode('detected_aspects').groupby(['detected_aspects', 'sentiment']).size().unstack(fill_value=0)\n",
        "aspect_counts['negative_ratio'] = aspect_counts['negative'] / (aspect_counts[['negative', 'positive']].sum(axis=1))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=aspect_counts.sort_values('negative_ratio', ascending=False).head(10),\n",
        "            x='negative_ratio', y=aspect_counts.sort_values('negative_ratio', ascending=False).head(10).index)\n",
        "plt.title(\"Top Aspects with Highest % of Negative Feedback\")\n",
        "plt.xlabel(\"% Negative Reviews\")\n",
        "plt.ylabel(\"Aspect\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OlC61y3OZhOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply aspect extraction across all reviews\n",
        "df['detected_aspects'] = df['customer_review'].apply(lambda x: extract_aspects(x, aspects))\n",
        "\n",
        "# Explode the dataframe so each row has one aspect per review\n",
        "aspect_sentiment = df.explode('detected_aspects').dropna(subset=['detected_aspects'])\n",
        "\n",
        "# Group by aspect and sentiment\n",
        "aspect_counts = aspect_sentiment.groupby(['detected_aspects', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "# Add total and negative ratio columns\n",
        "aspect_counts['total'] = aspect_counts.sum(axis=1)\n",
        "aspect_counts['negative_ratio'] = aspect_counts['negative'] / aspect_counts['total']\n",
        "aspect_counts = aspect_counts.sort_values(by='negative_ratio', ascending=False)\n",
        "\n",
        "# Show top 10 most negatively reviewed aspects\n",
        "print(\"Top Aspects by Negative Sentiment Ratio\")\n",
        "print(aspect_counts.head(10))"
      ],
      "metadata": {
        "id": "lrgUdioeh4kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aspect-Based Sentiment Analysis (ABSA)**"
      ],
      "metadata": {
        "id": "UE_Waw0Mi558"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch"
      ],
      "metadata": {
        "id": "50Paulz3Zjt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")"
      ],
      "metadata": {
        "id": "Uj_qILhszNd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for ABSA\n",
        "def analyze_aspect_sentiment(review, aspect):\n",
        "    inputs = tokenizer(f\"[CLS] {review} [SEP] {aspect} [SEP]\", return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "\n",
        "    sentiment = [\"negative\", \"neutral\", \"positive\"][probs.argmax()]\n",
        "    return sentiment"
      ],
      "metadata": {
        "id": "Tjrp_ev4jE83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_review = df.iloc[100]['customer_review']\n",
        "detected_aspects = extract_aspects(random_review, aspects)  # From your existing function\n",
        "\n",
        "print(\"Review:\", random_review)\n",
        "for aspect in detected_aspects:\n",
        "    sentiment = analyze_aspect_sentiment(random_review, aspect)\n",
        "    print(f\"- {aspect}: {sentiment}\")"
      ],
      "metadata": {
        "id": "ymA8BhUFlEpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_review = df.iloc[20]['customer_review']\n",
        "detected_aspects = extract_aspects(random_review, aspects)  # From your existing function\n",
        "\n",
        "print(\"Review:\", random_review)\n",
        "for aspect in detected_aspects:\n",
        "    sentiment = analyze_aspect_sentiment(random_review, aspect)\n",
        "    print(f\"- {aspect}: {sentiment}\")"
      ],
      "metadata": {
        "id": "kdcg_X_Xqp8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_review = df.iloc[105]['customer_review']\n",
        "detected_aspects = extract_aspects(random_review, aspects)  # From your existing function\n",
        "\n",
        "print(\"Review:\", random_review)\n",
        "for aspect in detected_aspects:\n",
        "    sentiment = analyze_aspect_sentiment(random_review, aspect)\n",
        "    print(f\"- {aspect}: {sentiment}\")"
      ],
      "metadata": {
        "id": "dUSMzYQww-gZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}